---
title: "Simulation"
output: pdf_document
---

# Preparing environment

```{r}
# clear environment
rm(list = ls())
```

```{r}
# load packages
library(dplyr) # for data handling
library(ordinal) # for building of ordinal mixed model
```

# Preparation of simulation

## Random data generation 

First, we write a function that generates random data, so that the simulation can be done multiple times. 

Random outcome variables are generated that adhere to the hypotheses. The hypotheses can be found below; here, $`++`$ indicates a strong positive effect, $`+`$ indicates a positive effect, $0$ indicates no effect, and $`-`$ indicates a negative effect.

- **negative evaluation in the main clause (neg_eval)**: increases acceptability of test sentence that contains this feature $(++)$
- **focus word in the subordinate clause (focus)**: increases acceptability of test sentence that contains this feature $(++)$
- **constituents following verb (constituent)**: increases acceptability of test sentence that contains this feature $(+)$
- **sequence of negative elements (sequence)**: reduces acceptability of test sentence that contains this feature $(-)$
- **zonder te or zonder dat (te_dat)**: has no effect on which sentences is perferred $(0)$
- **comparison with a minimally diff sentence (double_neg)**: reduces acceptability of test sentence $(-)$

```{r}
data.gen <- function(seed, npart = 30, ntest = 15, nmin = 20){
  set.seed(seed)
  check <- F # make sure there are differences in feature within all pairs
  
  while(check == F){
    # create participants 
    n_participants <- npart
    df_participants <- data.frame(
      person_ID = paste0("person_ID", 1:n_participants),
      age = round(rnorm(n_participants, 22, 3))
    )
    
    # create test-test pairs
    n_test <- ntest
    df_test <- data.frame(
      type = "test-test",
      neg_eval = sample(c(-1, 0, 1), n_test, replace = T),
      focus = sample(c(-1, 0, 1), n_test, replace = T),
      constituent = sample(c(-1, 0, 1), n_test, replace = T),
      sequence = sample(c(-1, 0, 1), n_test, replace = T),
      te_dat = sample(c(-1, 0, 1), n_test, replace = T),
      double_neg = rep(0, n_test)
    )
    
    df_test$pair_ID <- paste0("pair_", 1:n_test)
    
    # create test-minimally different pairs
    n_min <- nmin
    df_mindiff <- data.frame(
      type = "test-minimally different",
      neg_eval = rep(0, n_min),
      focus = rep(0, n_min),
      constituent = rep(0, n_min),
      sequence = rep(0, n_min),
      te_dat = rep(0, n_min),
      double_neg = rep(1, n_min)
    )
    
    df_mindiff$pair_ID <- paste0("pair_", (n_test + 1):(n_min + n_test))
    
    # create full dataset 
    expand.grid.df <- function(...) Reduce(function(...) merge(..., by=NULL), list(...))
    data_sent <- expand.grid.df(rbind(df_test, df_mindiff), data.frame(1:n_participants))[,-9]
    data_part <- expand.grid.df(df_participants, data.frame(1:(n_min + n_test)))[, -3] %>%
      arrange(person_ID)
    data <- cbind(data_part, data_sent)
    
    ###############################
    ### create outcome variable ###
    ###############################
    
    # determine threshold coefficients
    b01 <- 4
    b02 <- 1.5
    b03 <- -0.01
    b04 <- -1
    
    # determine fixed effects
    b1 <- 1.3 # negative evaluation in main clause
    b2 <- 1.1 # focus word in subordinate clause
    b3 <- 0.5 # constituent following a verb
    b4 <- - 0.7 # sequence of negative elements
    b5 <- 0 # zonder te vs. zonder dat 
    b6 <- - 1.5 # comparison with a minimally different sentence
    
    # add random effects 
    rand.part <- rnorm(n_participants) # random effects for participants
    rand.part <- sort(rep(rand.part, n_test + n_min))
    
    rand.sent <- rnorm(n_test + n_min) # random effects for sentence pairs
    rand.sent <- rep(rand.sent, n_participants)
    
    randomeffs <- rand.part + rand.sent 
    
    # add error terms
    errs <- rnorm(nrow(data))
    
    # calculate log odds 
    param_outcomes <- b1*data$neg_eval + b2*data$focus + b3*data$constituent + 
      b4*data$sequence + b5*data$te_dat + b6*data$double_neg
    
    logodds1 <- b01 + param_outcomes + randomeffs + errs
    logodds2 <- b02 + param_outcomes + randomeffs + errs
    logodds3 <- b03 + param_outcomes + randomeffs + errs
    logodds4 <- b04 + param_outcomes + randomeffs + errs
    
    # transform back to outcome variable 
    inv_logit <- function(logit) exp(logit) / (1 + exp(logit))
    
    prob_2to5 <- inv_logit(logodds1)
    prob_3to5 <- inv_logit(logodds2)
    prob_4to5 <- inv_logit(logodds3)
    prob_5 <- inv_logit(logodds4)
    
    prob_1 <- 1 - prob_2to5
    prob_2 <- prob_2to5 - prob_3to5
    prob_3 <- prob_3to5 - prob_4to5
    prob_4 <- prob_4to5 - prob_5
    
    y <- numeric(nrow(data))
    for(i in 1:nrow(data)){
      y[i] <- sample(c(1:5), 1, 
                     prob = c(prob_1[i], prob_2[i], prob_3[i], prob_4[i], prob_5[i]))
    }
    
    # add outcome to dataframe
    data$y <- y
    
    # check there are no sentences that contain the exact same features
    check <- sum(rowSums(abs(data[,4:9])) == 0) == 0
  }
  
  # make sure the columns are in the correct formats 
  data[,4:8] <- lapply(data[,4:8], as.numeric)
  data[,c(1, 9:11)] <- lapply(data[,c(1, 9:11)], as.factor)
    
  return(data)
}
```

## Descriptive statistics

It is useful to look at the distribution of the outcome variable to determine which link function is most suitable. So, we will write a function that generates descriptive statistics for a dataset. 

```{r}
# function for descriptive statistics
descr.stat <- function(data){
  # distribution of outcome variable
  hist(as.numeric(data$y), col = "steelblue",
       breaks = 15,
       xlab = "Likert scale",
       main = "")
}
```


## Model building 

Lastly, we will write a function that evaluations the ordinal mixed effects model. To do so, we evaluate the model with all the possible link functions, and choose the one that performs best, using Akaike's information criterion (AIC). The lowest AIC criterion is deemed to be the best fitting model. 

```{r}
# function that evaluations the model
mixed.model <- function(data){
  # fitting the model with different link functions
  mm_logit <- clmm(y ~ neg_eval + focus + constituent + sequence
                   + te_dat + double_neg + (1|person_ID) + (1|pair_ID), data = data,
                   link = "logit", 
                   threshold = "flexible")
  mm_loglog <- clmm(y ~ neg_eval + focus + constituent + sequence
                   + te_dat + double_neg + (1|person_ID) + (1|pair_ID), data = data,
                   link = "loglog", 
                   threshold = "flexible")
  mm_probit <- clmm(y ~ neg_eval + focus + constituent + sequence
                   + te_dat + double_neg + (1|person_ID) + (1|pair_ID), data = data,
                   link = "probit", 
                   threshold = "flexible")
  mm_cloglog <- clmm(y ~ neg_eval + focus + constituent + sequence
                   + te_dat + double_neg + (1|person_ID) + (1|pair_ID), data = data,
                   link = "cloglog", 
                   threshold = "flexible")
  # mm_cauchit <- clmm(y ~ neg_eval + focus + constituent + sequence
  #                   + te_dat + double_neg + (1|person_ID) + (1|pair_ID), data = data,
  #                   link = "cauchit", 
  #                  threshold = "flexible") # does not converge
  
  # calculating the AIC for each model 
  AICs <- c(mm_logit[[29]][5], # logit 
            mm_loglog[[29]][5], # loglog
            mm_probit[[29]][5], # probit
            mm_cloglog[[29]][5]) # cloglog
  
  # choosing the model with the lowest AIC
  if(which.min(AICs) == 1) mm_logit
  else if(which.min(AICs) == 2) mm_loglog
  else if(which.min(AICs) == 3) mm_probit
  else if(which.min(AICs) == 4) mm_cloglog
}
```


# Simulation

Now, we are equipped to simulate data and evaluate the outcomes. To do so, we generate $100$ datasets, and take a look at some descriptive statistics. 

## Generate 100 datasets

```{r}
# draw 100 random seeds
set.seed(123)
seeds <- unique(sample(100000, 100, replace = F))

# create list to save all datasets
datasets <- vector(mode = "list", length = length(seeds))

# generate datasets
for(i in 1:length(seeds)){
  datasets[[i]] <- data.gen(seeds[i])
}
```

## Fit the model for all datasets

```{r, echo = F}
# create vectors to save the outcomes
descr.statistics <- vector(mode = "list", length = length(datasets))
models <- vector(mode = "list", length = length(datasets))

for(i in 1:length(datasets)){
  descr.statistics[[i]] <- descr.stat(datasets[[i]])
  models[[i]] <- mixed.model(datasets[[i]])
}
```

## Descriptive statistics on simulations

Lastly, we will briefly look at the outcome of our simulations. More specifically, we look at how often each function (logit, probit, loglog, and cloglog) performed best. Thereafter, we look at the proportion of times each sentential feature was found to be statistically significant; this represents the power for the above chosen effect sizes (regression coefficients) for the given number of participants.

```{r}
# which functions are preferred 
table(as.character(sapply(models, "[", 17)))

# power
pvals <- lapply(models, summary)
pvals <- lapply(pvals, coef)
pvals <- sapply(pvals, "[", c(35:40))
rownames(pvals) <- c("neg_eval", "focus", "constituent", "sequence", "te_dat", "double_neg")

rowMeans(pvals < 0.05)
```

# Outcomes of the simulation for the results section 

For the results section, we consider only the firstly generated dataset. The descriptive statistics and outcomes of this model can be found below. 

```{r}
# get data as a new dataframe
df <- datasets[[1]]
df_model <- models[[1]]

# look at the outcomes 
head(df)
descr.stat(df)
summary(df_model)

summary((df %>% distinct(person_ID, age))$age)

# get the odds ratios 
# log odds ratio
round(coef(summary(df_model))[5:10,1], 3)

# odds ratio
round(exp(coef(summary(df_model))[5:10,1]), 3)

# probabiliites
round(exp(coef(summary(df_model))[5:10,1])/(1+ exp(coef(summary(df_model))[5:10,1])), 3)

# p-values of threshold coefficients 
round(coef(summary(df_model))[1:4,4], 3)
```

# Useful links
expand.grid function: 
https://stackoverflow.com/questions/11693599/alternative-to-expand-grid-for-data-frames 

simulation of outcome: 
https://stats.stackexchange.com/questions/321770/simulating-data-for-an-ordered-logit-model
